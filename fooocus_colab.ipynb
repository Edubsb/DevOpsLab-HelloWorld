{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Edubsb/DevOpsLab-HelloWorld/blob/master/fooocus_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjYy0F2gZIPR",
        "outputId": "12584653-79b3-46cb-ea7e-e1f4204c3d46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pygit2==1.15.1\n",
            "  Downloading pygit2-1.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: cffi>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from pygit2==1.15.1) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.16.0->pygit2==1.15.1) (2.22)\n",
            "Downloading pygit2-1.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pygit2\n",
            "  Attempting uninstall: pygit2\n",
            "    Found existing installation: pygit2 1.17.0\n",
            "    Uninstalling pygit2-1.17.0:\n",
            "      Successfully uninstalled pygit2-1.17.0\n",
            "Successfully installed pygit2-1.15.1\n",
            "/content\n",
            "Cloning into 'Fooocus'...\n",
            "remote: Enumerating objects: 6725, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 6725 (delta 2), reused 0 (delta 0), pack-reused 6721 (from 2)\u001b[K\n",
            "Receiving objects: 100% (6725/6725), 33.27 MiB | 32.88 MiB/s, done.\n",
            "Resolving deltas: 100% (3875/3875), done.\n",
            "/content/Fooocus\n",
            "Already up-to-date\n",
            "Update succeeded.\n",
            "[System ARGV] ['entry_with_update.py', '--share', '--always-high-vram']\n",
            "Python 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n",
            "Fooocus version: 2.5.5\n",
            "Error checking version for torchsde: No package metadata was found for torchsde\n",
            "Installing requirements\n",
            "[Cleanup] Attempting to delete content of temp dir /tmp/fooocus\n",
            "[Cleanup] Cleanup successful\n",
            "Downloading: \"https://huggingface.co/lllyasviel/misc/resolve/main/xlvaeapp.pth\" to /content/Fooocus/models/vae_approx/xlvaeapp.pth\n",
            "\n",
            "100% 209k/209k [00:00<00:00, 10.7MB/s]\n",
            "Downloading: \"https://huggingface.co/lllyasviel/misc/resolve/main/vaeapp_sd15.pt\" to /content/Fooocus/models/vae_approx/vaeapp_sd15.pth\n",
            "\n",
            "100% 209k/209k [00:00<00:00, 10.0MB/s]\n",
            "Downloading: \"https://huggingface.co/mashb1t/misc/resolve/main/xl-to-v1_interposer-v4.0.safetensors\" to /content/Fooocus/models/vae_approx/xl-to-v1_interposer-v4.0.safetensors\n",
            "\n",
            "100% 5.40M/5.40M [00:00<00:00, 46.8MB/s]\n",
            "Downloading: \"https://huggingface.co/lllyasviel/misc/resolve/main/fooocus_expansion.bin\" to /content/Fooocus/models/prompt_expansion/fooocus_expansion/pytorch_model.bin\n",
            "\n",
            "100% 335M/335M [00:01<00:00, 230MB/s]\n",
            "Downloading: \"https://huggingface.co/lllyasviel/fav_models/resolve/main/fav/juggernautXL_v8Rundiffusion.safetensors\" to /content/Fooocus/models/checkpoints/juggernautXL_v8Rundiffusion.safetensors\n",
            "\n",
            "100% 6.62G/6.62G [00:35<00:00, 198MB/s]\n",
            "Downloading: \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_offset_example-lora_1.0.safetensors\" to /content/Fooocus/models/loras/sd_xl_offset_example-lora_1.0.safetensors\n",
            "\n",
            "100% 47.3M/47.3M [00:00<00:00, 307MB/s]\n",
            "Total VRAM 15095 MB, total RAM 12979 MB\n",
            "Set vram state to: HIGH_VRAM\n",
            "Always offload VRAM\n",
            "Device: cuda:0 Tesla T4 : native\n",
            "VAE dtype: torch.float32\n",
            "Using pytorch cross attention\n",
            "Refiner unloaded.\n",
            "IMPORTANT: You are using gradio version 3.41.2, however version 4.44.1 is available, please upgrade.\n",
            "--------\n",
            "Running on local URL:  http://127.0.0.1:7865\n",
            "Running on public URL: https://73f513d001cfa7c5ec.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "model_type EPS\n",
            "UNet ADM Dimension 2816\n",
            "Using pytorch attention in VAE\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "Using pytorch attention in VAE\n",
            "extra {'cond_stage_model.clip_l.logit_scale', 'cond_stage_model.clip_l.text_projection'}\n",
            "left over keys: dict_keys(['cond_stage_model.clip_l.transformer.text_model.embeddings.position_ids'])\n",
            "loaded straight to GPU\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "Base model loaded: /content/Fooocus/models/checkpoints/juggernautXL_v8Rundiffusion.safetensors\n",
            "VAE loaded: None\n",
            "Request to load LoRAs [('sd_xl_offset_example-lora_1.0.safetensors', 0.1)] for model [/content/Fooocus/models/checkpoints/juggernautXL_v8Rundiffusion.safetensors].\n",
            "Loaded LoRA [/content/Fooocus/models/loras/sd_xl_offset_example-lora_1.0.safetensors] for UNet [/content/Fooocus/models/checkpoints/juggernautXL_v8Rundiffusion.safetensors] with 788 keys at weight 0.1.\n",
            "Fooocus V2 Expansion: Vocab with 642 words.\n",
            "Fooocus Expansion engine loaded for cuda:0, use_fp16 = True.\n",
            "Requested to load SDXLClipModel\n",
            "Requested to load GPT2LMHeadModel\n",
            "Loading 2 new models\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.59 seconds\n",
            "2025-02-25 23:14:22.883400: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1740525263.145749    1728 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1740525263.217149    1728 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-25 23:14:23.808217: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Started worker with PID 720\n",
            "App started successful. Use the app with http://127.0.0.1:7865/ or 127.0.0.1:7865 or https://73f513d001cfa7c5ec.gradio.live\n",
            "Loaded preset: /content/Fooocus/presets/realistic.json\n",
            "Downloading: \"https://huggingface.co/lllyasviel/fav_models/resolve/main/fav/realisticStockPhoto_v20.safetensors\" to /content/Fooocus/models/checkpoints/realisticStockPhoto_v20.safetensors\n",
            "\n",
            "100% 6.46G/6.46G [01:45<00:00, 65.5MB/s]\n",
            "Downloading: \"https://huggingface.co/mashb1t/fav_models/resolve/main/fav/SDXL_FILM_PHOTOGRAPHY_STYLE_V1.safetensors\" to /content/Fooocus/models/loras/SDXL_FILM_PHOTOGRAPHY_STYLE_V1.safetensors\n",
            "\n",
            "100% 870M/870M [00:25<00:00, 35.9MB/s]\n",
            "Loaded preset: /content/Fooocus/presets/realistic.json\n",
            "[Parameters] Adaptive CFG = 7\n",
            "[Parameters] CLIP Skip = 2\n",
            "[Parameters] Sharpness = 2\n",
            "[Parameters] ControlNet Softness = 0.25\n",
            "[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n",
            "[Parameters] Seed = 0\n",
            "[Parameters] CFG = 3\n",
            "[Fooocus] Loading control models ...\n",
            "[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n",
            "[Parameters] Steps = 30 - 15\n",
            "[Fooocus] Initializing ...\n",
            "[Fooocus] Loading models ...\n",
            "Refiner unloaded.\n",
            "model_type EPS\n",
            "UNet ADM Dimension 2816\n",
            "Using pytorch attention in VAE\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "Using pytorch attention in VAE\n",
            "extra {'cond_stage_model.clip_l.logit_scale', 'cond_stage_model.clip_l.text_projection'}\n",
            "left over keys: dict_keys(['cond_stage_model.clip_l.transformer.text_model.embeddings.position_ids'])\n",
            "loaded straight to GPU\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 2.31 seconds\n",
            "Base model loaded: /content/Fooocus/models/checkpoints/realisticStockPhoto_v20.safetensors\n",
            "VAE loaded: None\n",
            "Request to load LoRAs [('SDXL_FILM_PHOTOGRAPHY_STYLE_V1.safetensors', 0.25)] for model [/content/Fooocus/models/checkpoints/realisticStockPhoto_v20.safetensors].\n",
            "Loaded LoRA [/content/Fooocus/models/loras/SDXL_FILM_PHOTOGRAPHY_STYLE_V1.safetensors] for UNet [/content/Fooocus/models/checkpoints/realisticStockPhoto_v20.safetensors] with 722 keys at weight 0.25.\n",
            "Loaded LoRA [/content/Fooocus/models/loras/SDXL_FILM_PHOTOGRAPHY_STYLE_V1.safetensors] for CLIP [/content/Fooocus/models/checkpoints/realisticStockPhoto_v20.safetensors] with 264 keys at weight 0.25.\n",
            "Requested to load SDXLClipModel\n",
            "Requested to load GPT2LMHeadModel\n",
            "Loading 2 new models\n",
            "[Fooocus Model Management] Moving model(s) has taken 1.66 seconds\n",
            "[Fooocus] Processing prompts ...\n",
            "[Fooocus] Preparing Fooocus text #1 ...\n",
            "[Prompt Expansion] \"melhor qualidade, ultra-detalhado, 8k, realista, beautiful gorgeous, dramatic, detailed, warm color, light, winning, grand romantic, artistic, fine detail, intricate, elegant, sharp focus, trendy, fancy, amazing, symmetry, determined, fabulous, perfect, colorful, great composition, professional, ambient, dynamic, vibrant\n",
            "[Fooocus] Preparing Fooocus text #2 ...\n",
            "[Prompt Expansion] \"melhor qualidade, ultra-detalhado, 8k, realista, gorgeous, cinematic, dynamic, detailed, elegant, professional, beautiful, dramatic, breathtaking, artistic, sharp focus, cool color, perfect, pretty, epic, coherent, awesome, colorful, best, ambient, deep light, great composition, amazing, pure, symmetry, full\n",
            "[Fooocus] Encoding positive #1 ...\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.11 seconds\n",
            "[Fooocus] Encoding positive #2 ...\n",
            "[Fooocus] Encoding negative #1 ...\n",
            "[Fooocus] Encoding negative #2 ...\n",
            "[Parameters] Denoising Strength = 1.0\n",
            "[Parameters] Initial Latent shape: Image Space (1152, 896)\n",
            "Preparation time: 53.45 seconds\n",
            "Using karras scheduler.\n",
            "[Fooocus] Preparing task 1/2 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 2.30 seconds\n",
            "100% 30/30 [00:28<00:00,  1.06it/s]\n",
            "Requested to load AutoencoderKL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.25 seconds\n",
            "[Fooocus] Saving image 1/2 to system ...\n",
            "Image generated with private log at: /content/Fooocus/outputs/2025-02-26/log.html\n",
            "Generating and saving time: 33.51 seconds\n",
            "[Fooocus] Preparing task 2/2 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.72 seconds\n",
            "100% 30/30 [00:28<00:00,  1.05it/s]\n",
            "Requested to load AutoencoderKL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.23 seconds\n",
            "[Fooocus] Saving image 2/2 to system ...\n",
            "Image generated with private log at: /content/Fooocus/outputs/2025-02-26/log.html\n",
            "Generating and saving time: 32.18 seconds\n",
            "[Enhance] Skipping, preconditions aren't met\n",
            "Processing time (total): 65.69 seconds\n",
            "Requested to load SDXLClipModel\n",
            "Requested to load GPT2LMHeadModel\n",
            "Loading 2 new models\n",
            "Total time: 119.19 seconds\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.96 seconds\n",
            "[Parameters] Adaptive CFG = 7\n",
            "[Parameters] CLIP Skip = 2\n",
            "[Parameters] Sharpness = 2\n",
            "[Parameters] ControlNet Softness = 0.25\n",
            "[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n",
            "[Parameters] Seed = 0\n",
            "[Parameters] CFG = 3\n",
            "[Fooocus] Loading control models ...\n",
            "[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n",
            "[Parameters] Steps = 60 - 30\n",
            "[Fooocus] Initializing ...\n",
            "[Fooocus] Loading models ...\n",
            "Refiner unloaded.\n",
            "[Fooocus] Processing prompts ...\n",
            "[Fooocus] Preparing Fooocus text #1 ...\n",
            "[Prompt Expansion] cachorro, cinematic, highly detailed, sharp focus, ambient light, intricate, innocent, elegant, complex, magical, enchanted, dramatic, background glowing, colorful, deep vibrant, color, illuminated, determined, very coherent, symmetry, strong, full, artistic, fair, true colors, romantic, pretty, peaceful, rational, cute, loving, caring, generous\n",
            "[Fooocus] Preparing Fooocus text #2 ...\n",
            "[Prompt Expansion] cachorro, colorful, cinematic, intricate, detailed, very coherent, sharp focus, elegant, color, cool, futuristic, background, shiny, highly decorated, vivid, complex, dramatic ambient, full dynamic composition, professional, glowing, magical, romantic, amazing, pure, symmetry, illuminated, perfect, light, fine detail, pretty, clear, crisp, extremely\n",
            "[Fooocus] Encoding positive #1 ...\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.12 seconds\n",
            "[Fooocus] Encoding positive #2 ...\n",
            "[Fooocus] Encoding negative #1 ...\n",
            "[Fooocus] Encoding negative #2 ...\n",
            "[Parameters] Denoising Strength = 1.0\n",
            "[Parameters] Initial Latent shape: Image Space (1152, 896)\n",
            "Preparation time: 2.50 seconds\n",
            "Using karras scheduler.\n",
            "[Fooocus] Preparing task 1/2 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.89 seconds\n",
            "100% 60/60 [00:53<00:00,  1.12it/s]\n",
            "Requested to load AutoencoderKL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.24 seconds\n",
            "[Fooocus] Saving image 1/2 to system ...\n",
            "Image generated with private log at: /content/Fooocus/outputs/2025-02-26/log.html\n",
            "Generating and saving time: 57.09 seconds\n",
            "[Fooocus] Preparing task 2/2 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.80 seconds\n",
            "100% 60/60 [00:55<00:00,  1.08it/s]\n",
            "Requested to load AutoencoderKL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.24 seconds\n",
            "[Fooocus] Saving image 2/2 to system ...\n",
            "Image generated with private log at: /content/Fooocus/outputs/2025-02-26/log.html\n",
            "Generating and saving time: 59.28 seconds\n",
            "[Fooocus] Processing enhance ...\n",
            "Enhancement image time: 0.00 seconds\n",
            "Enhancement image time: 0.00 seconds\n",
            "Processing time (total): 116.37 seconds\n",
            "Requested to load SDXLClipModel\n",
            "Requested to load GPT2LMHeadModel\n",
            "Loading 2 new models\n",
            "Total time: 118.93 seconds\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.96 seconds\n",
            "Enter LCM mode.\n",
            "[Fooocus] Downloading LCM components ...\n",
            "Downloading: \"https://huggingface.co/lllyasviel/misc/resolve/main/sdxl_lcm_lora.safetensors\" to /content/Fooocus/models/loras/sdxl_lcm_lora.safetensors\n",
            "\n",
            "100% 376M/376M [00:06<00:00, 56.4MB/s]\n",
            "[Parameters] Adaptive CFG = 1.0\n",
            "[Parameters] CLIP Skip = 2\n",
            "[Parameters] Sharpness = 0.0\n",
            "[Parameters] ControlNet Softness = 0.25\n",
            "[Parameters] ADM Scale = 1.0 : 1.0 : 0.0\n",
            "[Parameters] Seed = 0\n",
            "[Parameters] CFG = 1.0\n",
            "[Fooocus] Loading control models ...\n",
            "[Parameters] Sampler = lcm - lcm\n",
            "[Parameters] Steps = 8 - 8\n",
            "[Fooocus] Initializing ...\n",
            "[Fooocus] Loading models ...\n",
            "Refiner unloaded.\n",
            "Request to load LoRAs [('SDXL_FILM_PHOTOGRAPHY_STYLE_V1.safetensors', 0.25), ('sdxl_lcm_lora.safetensors', 1.0)] for model [/content/Fooocus/models/checkpoints/realisticStockPhoto_v20.safetensors].\n",
            "Loaded LoRA [/content/Fooocus/models/loras/SDXL_FILM_PHOTOGRAPHY_STYLE_V1.safetensors] for UNet [/content/Fooocus/models/checkpoints/realisticStockPhoto_v20.safetensors] with 722 keys at weight 0.25.\n",
            "Loaded LoRA [/content/Fooocus/models/loras/SDXL_FILM_PHOTOGRAPHY_STYLE_V1.safetensors] for CLIP [/content/Fooocus/models/checkpoints/realisticStockPhoto_v20.safetensors] with 264 keys at weight 0.25.\n",
            "Loaded LoRA [/content/Fooocus/models/loras/sdxl_lcm_lora.safetensors] for UNet [/content/Fooocus/models/checkpoints/realisticStockPhoto_v20.safetensors] with 788 keys at weight 1.0.\n",
            "Requested to load SDXLClipModel\n",
            "Loading 1 new model\n",
            "unload clone 1\n",
            "[Fooocus Model Management] Moving model(s) has taken 1.08 seconds\n",
            "[Fooocus] Processing prompts ...\n",
            "[Fooocus] Preparing Fooocus text #1 ...\n",
            "[Prompt Expansion] cachorro, cinematic, highly detailed, sharp focus, ambient light, intricate, innocent, elegant, complex, magical, enchanted, dramatic, background glowing, colorful, deep vibrant, color, illuminated, determined, very coherent, symmetry, strong, full, artistic, fair, true colors, romantic, pretty, peaceful, rational, cute, loving, caring, generous\n",
            "[Fooocus] Preparing Fooocus text #2 ...\n",
            "[Prompt Expansion] cachorro, colorful, cinematic, intricate, detailed, very coherent, sharp focus, elegant, color, cool, futuristic, background, shiny, highly decorated, vivid, complex, dramatic ambient, full dynamic composition, professional, glowing, magical, romantic, amazing, pure, symmetry, illuminated, perfect, light, fine detail, pretty, clear, crisp, extremely\n",
            "[Fooocus] Encoding positive #1 ...\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.11 seconds\n",
            "[Fooocus] Encoding positive #2 ...\n",
            "[Parameters] Denoising Strength = 1.0\n",
            "[Parameters] Initial Latent shape: Image Space (1152, 896)\n",
            "Preparation time: 10.54 seconds\n",
            "Using sgm_uniform scheduler.\n",
            "[Fooocus] Preparing task 1/2 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.39970144629478455, sigma_max = 14.614640235900879\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 1.29 seconds\n",
            "100% 8/8 [00:03<00:00,  2.29it/s]\n",
            "Requested to load AutoencoderKL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.31 seconds\n",
            "[Fooocus] Saving image 1/2 to system ...\n",
            "Image generated with private log at: /content/Fooocus/outputs/2025-02-26/log.html\n",
            "Generating and saving time: 7.63 seconds\n",
            "[Fooocus] Preparing task 2/2 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.39970144629478455, sigma_max = 14.614640235900879\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 1.09 seconds\n",
            "100% 8/8 [00:03<00:00,  2.31it/s]\n",
            "Requested to load AutoencoderKL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.26 seconds\n",
            "[Fooocus] Saving image 2/2 to system ...\n",
            "Image generated with private log at: /content/Fooocus/outputs/2025-02-26/log.html\n",
            "Generating and saving time: 7.29 seconds\n",
            "[Fooocus] Processing enhance ...\n",
            "Enhancement image time: 0.00 seconds\n",
            "Enhancement image time: 0.00 seconds\n",
            "Processing time (total): 14.92 seconds\n",
            "Requested to load SDXLClipModel\n",
            "Requested to load GPT2LMHeadModel\n",
            "Loading 2 new models\n",
            "Total time: 25.53 seconds\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.92 seconds\n",
            "Loaded preset: /content/Fooocus/presets/realistic.json\n",
            "[Parameters] Adaptive CFG = 7\n",
            "[Parameters] CLIP Skip = 2\n",
            "[Parameters] Sharpness = 2\n",
            "[Parameters] ControlNet Softness = 0.25\n",
            "[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n",
            "[Parameters] Seed = 8711387064946514083\n",
            "[Parameters] CFG = 3\n",
            "[Fooocus] Loading control models ...\n",
            "[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n",
            "[Parameters] Steps = 30 - 16\n",
            "[Fooocus] Initializing ...\n",
            "[Fooocus] Loading models ...\n",
            "model_type EPS\n",
            "UNet ADM Dimension 2816\n",
            "Using pytorch attention in VAE\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "Using pytorch attention in VAE\n",
            "extra {'cond_stage_model.clip_l.logit_scale', 'cond_stage_model.clip_l.text_projection'}\n",
            "left over keys: dict_keys(['cond_stage_model.clip_l.transformer.text_model.embeddings.position_ids'])\n",
            "loaded straight to GPU\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.69 seconds\n",
            "Refiner model loaded: /content/Fooocus/models/checkpoints/juggernautXL_v8Rundiffusion.safetensors\n",
            "Request to load LoRAs [('SDXL_FILM_PHOTOGRAPHY_STYLE_V1.safetensors', 0.25), ('sd_xl_offset_example-lora_1.0.safetensors', 1.0)] for model [/content/Fooocus/models/checkpoints/realisticStockPhoto_v20.safetensors].\n",
            "Loaded LoRA [/content/Fooocus/models/loras/SDXL_FILM_PHOTOGRAPHY_STYLE_V1.safetensors] for UNet [/content/Fooocus/models/checkpoints/realisticStockPhoto_v20.safetensors] with 722 keys at weight 0.25.\n",
            "Loaded LoRA [/content/Fooocus/models/loras/SDXL_FILM_PHOTOGRAPHY_STYLE_V1.safetensors] for CLIP [/content/Fooocus/models/checkpoints/realisticStockPhoto_v20.safetensors] with 264 keys at weight 0.25.\n",
            "Loaded LoRA [/content/Fooocus/models/loras/sd_xl_offset_example-lora_1.0.safetensors] for UNet [/content/Fooocus/models/checkpoints/realisticStockPhoto_v20.safetensors] with 788 keys at weight 1.0.\n",
            "Request to load LoRAs [('SDXL_FILM_PHOTOGRAPHY_STYLE_V1.safetensors', 0.25), ('sd_xl_offset_example-lora_1.0.safetensors', 1.0)] for model [/content/Fooocus/models/checkpoints/juggernautXL_v8Rundiffusion.safetensors].\n",
            "Loaded LoRA [/content/Fooocus/models/loras/SDXL_FILM_PHOTOGRAPHY_STYLE_V1.safetensors] for UNet [/content/Fooocus/models/checkpoints/juggernautXL_v8Rundiffusion.safetensors] with 722 keys at weight 0.25.\n",
            "Loaded LoRA [/content/Fooocus/models/loras/sd_xl_offset_example-lora_1.0.safetensors] for UNet [/content/Fooocus/models/checkpoints/juggernautXL_v8Rundiffusion.safetensors] with 788 keys at weight 1.0.\n",
            "Requested to load SDXLClipModel\n",
            "Requested to load GPT2LMHeadModel\n",
            "Loading 2 new models\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.89 seconds\n",
            "[Fooocus] Processing prompts ...\n",
            "[Fooocus] Preparing Fooocus text #1 ...\n",
            "[Prompt Expansion] batmam, de copo inteiro, futuristic, glowing, highly detailed, complex, background, colorful, surreal, dramatic ambient, sharp focus, epic composition, magical atmosphere, dynamic light, professional fine detail, very coherent, beautiful, stunning, symmetry, perfect, full color, pretty, cinematic, great thought, aesthetic, best, whole amazing, awesome\n",
            "[Fooocus] Preparing Fooocus text #2 ...\n",
            "[Prompt Expansion] batmam, de copo inteiro, dramatic ambient light, dynamic composition, highly detailed, elegant, colorful, deep aesthetic, very inspirational, beautiful, fine detail, intricate, cinematic, complex, stunning, sharp focus, professional background, fancy strong, winning creative, bright, sunny, hopeful, pretty, cheerful, color, inspired, vibrant, illuminated, amazing\n",
            "[Fooocus] Encoding positive #1 ...\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.11 seconds\n",
            "[Fooocus] Encoding positive #2 ...\n",
            "[Fooocus] Encoding negative #1 ...\n",
            "[Fooocus] Encoding negative #2 ...\n",
            "[Parameters] Denoising Strength = 1.0\n",
            "[Parameters] Initial Latent shape: Image Space (1152, 896)\n",
            "Preparation time: 48.21 seconds\n",
            "Using karras scheduler.\n",
            "[Fooocus] Preparing task 1/2 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "ERROR diffusion_model.input_blocks.8.0.in_layers.2.weight CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 26.12 MiB is free. Process 7002 has 14.71 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 472.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "ERROR diffusion_model.input_blocks.8.0.out_layers.3.weight CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 26.12 MiB is free. Process 7002 has 14.71 GiB memory in use. Of the allocated memory 14.15 GiB is allocated by PyTorch, and 441.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "ERROR diffusion_model.middle_block.0.in_layers.2.weight CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 26.12 MiB is free. Process 7002 has 14.71 GiB memory in use. Of the allocated memory 14.13 GiB is allocated by PyTorch, and 469.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "ERROR diffusion_model.middle_block.0.out_layers.3.weight CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 26.12 MiB is free. Process 7002 has 14.71 GiB memory in use. Of the allocated memory 14.16 GiB is allocated by PyTorch, and 438.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 1471, in worker\n",
            "    handler(task)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 1286, in handler\n",
            "    imgs, img_paths, current_progress = process_task(all_steps, async_task, callback, controlnet_canny_path,\n",
            "                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 295, in process_task\n",
            "    imgs = pipeline.process_diffusion(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/modules/default_pipeline.py\", line 379, in process_diffusion\n",
            "    sampled_latent = core.ksampler(\n",
            "                     ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/modules/core.py\", line 310, in ksampler\n",
            "    samples = ldm_patched.modules.sample.sample(model,\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/ldm_patched/modules/sample.py\", line 93, in sample\n",
            "    real_model, positive_copy, negative_copy, noise_mask, models = prepare_sampling(model, noise.shape, positive, negative, noise_mask)\n",
            "                                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/ldm_patched/modules/sample.py\", line 86, in prepare_sampling\n",
            "    ldm_patched.modules.model_management.load_models_gpu([model] + models, model.memory_required([noise_shape[0] * 2] + list(noise_shape[1:])) + inference_memory)\n",
            "  File \"/content/Fooocus/modules/patch.py\", line 447, in patched_load_models_gpu\n",
            "    y = ldm_patched.modules.model_management.load_models_gpu_origin(*args, **kwargs)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/ldm_patched/modules/model_management.py\", line 437, in load_models_gpu\n",
            "    cur_loaded_model = loaded_model.model_load(lowvram_model_memory)\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/ldm_patched/modules/model_management.py\", line 304, in model_load\n",
            "    raise e\n",
            "  File \"/content/Fooocus/ldm_patched/modules/model_management.py\", line 300, in model_load\n",
            "    self.real_model = self.model.patch_model(device_to=patch_model_to) #TODO: do something with loras and offloading to CPU\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/ldm_patched/modules/model_patcher.py\", line 202, in patch_model\n",
            "    out_weight = self.calculate_weight(self.patches[key], temp_weight, key).to(weight.dtype)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 26.12 MiB is free. Process 7002 has 14.71 GiB memory in use. Of the allocated memory 14.16 GiB is allocated by PyTorch, and 438.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Total time: 49.81 seconds\n",
            "[Parameters] Adaptive CFG = 7\n",
            "[Parameters] CLIP Skip = 2\n",
            "[Parameters] Sharpness = 2\n",
            "[Parameters] ControlNet Softness = 0.25\n",
            "[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n",
            "[Parameters] Seed = 6257864887116403594\n",
            "[Parameters] CFG = 3\n",
            "[Fooocus] Loading control models ...\n",
            "[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n",
            "[Parameters] Steps = 30 - 16\n",
            "[Fooocus] Initializing ...\n",
            "[Fooocus] Loading models ...\n",
            "Requested to load SDXLClipModel\n",
            "Requested to load GPT2LMHeadModel\n",
            "Loading 2 new models\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.85 seconds\n",
            "[Fooocus] Processing prompts ...\n",
            "[Fooocus] Preparing Fooocus text #1 ...\n",
            "[Prompt Expansion] batmam, de copo inteiro, color, epic, extremely detailed, fantastic composition, cinematic light, dynamic, stunning, highly intricate, elegant, amazing detail, professional fine, complex, cool, perfect, best, futuristic, sharp focus, great colors, beautiful, symmetry, vivid, illuminated, atmosphere, optimistic, warm, vibrant, positive, glowing\n",
            "[Fooocus] Preparing Fooocus text #2 ...\n",
            "[Prompt Expansion] batmam, de copo inteiro, futuristic, elegant, highly detailed, complex, sharp focus, colorful, surreal, symmetry, dramatic light, ambient background, professional, unique, stunning, beautiful, best, expressive, lucid, color, thought, perfect, epic, romantic, enhanced, pure, illuminated, cinematic, artistic, vibrant, awesome, cool\n",
            "[Fooocus] Encoding positive #1 ...\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.11 seconds\n",
            "[Fooocus] Encoding positive #2 ...\n",
            "[Fooocus] Encoding negative #1 ...\n",
            "[Fooocus] Encoding negative #2 ...\n",
            "[Parameters] Denoising Strength = 1.0\n",
            "[Parameters] Initial Latent shape: Image Space (1152, 896)\n",
            "Preparation time: 3.53 seconds\n",
            "Using karras scheduler.\n",
            "[Fooocus] Preparing task 1/2 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "ERROR diffusion_model.input_blocks.8.0.in_layers.2.weight CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 26.12 MiB is free. Process 7002 has 14.71 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 471.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "ERROR diffusion_model.input_blocks.8.0.out_layers.3.weight CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 26.12 MiB is free. Process 7002 has 14.71 GiB memory in use. Of the allocated memory 14.15 GiB is allocated by PyTorch, and 440.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "ERROR diffusion_model.middle_block.0.in_layers.2.weight CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 26.12 MiB is free. Process 7002 has 14.71 GiB memory in use. Of the allocated memory 14.13 GiB is allocated by PyTorch, and 468.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "ERROR diffusion_model.middle_block.0.out_layers.3.weight CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 26.12 MiB is free. Process 7002 has 14.71 GiB memory in use. Of the allocated memory 14.16 GiB is allocated by PyTorch, and 437.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 1471, in worker\n",
            "    handler(task)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 1286, in handler\n",
            "    imgs, img_paths, current_progress = process_task(all_steps, async_task, callback, controlnet_canny_path,\n",
            "                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 295, in process_task\n",
            "    imgs = pipeline.process_diffusion(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/modules/default_pipeline.py\", line 379, in process_diffusion\n",
            "    sampled_latent = core.ksampler(\n",
            "                     ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/modules/core.py\", line 310, in ksampler\n",
            "    samples = ldm_patched.modules.sample.sample(model,\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/ldm_patched/modules/sample.py\", line 93, in sample\n",
            "    real_model, positive_copy, negative_copy, noise_mask, models = prepare_sampling(model, noise.shape, positive, negative, noise_mask)\n",
            "                                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/ldm_patched/modules/sample.py\", line 86, in prepare_sampling\n",
            "    ldm_patched.modules.model_management.load_models_gpu([model] + models, model.memory_required([noise_shape[0] * 2] + list(noise_shape[1:])) + inference_memory)\n",
            "  File \"/content/Fooocus/modules/patch.py\", line 447, in patched_load_models_gpu\n",
            "    y = ldm_patched.modules.model_management.load_models_gpu_origin(*args, **kwargs)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/ldm_patched/modules/model_management.py\", line 437, in load_models_gpu\n",
            "    cur_loaded_model = loaded_model.model_load(lowvram_model_memory)\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/ldm_patched/modules/model_management.py\", line 304, in model_load\n",
            "    raise e\n",
            "  File \"/content/Fooocus/ldm_patched/modules/model_management.py\", line 300, in model_load\n",
            "    self.real_model = self.model.patch_model(device_to=patch_model_to) #TODO: do something with loras and offloading to CPU\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/ldm_patched/modules/model_patcher.py\", line 202, in patch_model\n",
            "    out_weight = self.calculate_weight(self.patches[key], temp_weight, key).to(weight.dtype)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 26.12 MiB is free. Process 7002 has 14.71 GiB memory in use. Of the allocated memory 14.16 GiB is allocated by PyTorch, and 438.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Total time: 4.73 seconds\n",
            "[Parameters] Adaptive CFG = 7\n",
            "[Parameters] CLIP Skip = 2\n",
            "[Parameters] Sharpness = 2\n",
            "[Parameters] ControlNet Softness = 0.25\n",
            "[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n",
            "[Parameters] Seed = 6484518446467719889\n",
            "[Parameters] CFG = 3\n",
            "[Fooocus] Loading control models ...\n",
            "[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n",
            "[Parameters] Steps = 30 - 16\n",
            "[Fooocus] Initializing ...\n",
            "[Fooocus] Loading models ...\n",
            "Requested to load SDXLClipModel\n",
            "Requested to load GPT2LMHeadModel\n",
            "Loading 2 new models\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.82 seconds\n",
            "[Fooocus] Processing prompts ...\n",
            "[Fooocus] Preparing Fooocus text #1 ...\n",
            "[Prompt Expansion] Batmam, de copo inteiro, colorful, surreal, dramatic ambient, highly detailed, very sharp focus, fine light, shining, epic composition, magical atmosphere, cinematic, dynamic, vivid, beautiful, unique, professional, inspiring, perfect, best, artistic, pure, hopeful, cute, determined, color, extremely inspired, gigantic, amazing, creative\n",
            "[Fooocus] Preparing Fooocus text #2 ...\n",
            "[Prompt Expansion] Batmam, de copo inteiro, colorful, surreal, dramatic ambient, highly detailed, real, dynamic, sharp focus, beautiful, elegant, intricate, perfect light, professional color, best, romantic, unique, cute, charming, pretty, peaceful, tranquil, thought, full, bright, illuminated, shining, deep, aesthetic, great composition,, creative\n",
            "[Fooocus] Encoding positive #1 ...\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.12 seconds\n",
            "[Fooocus] Encoding positive #2 ...\n",
            "[Fooocus] Encoding negative #1 ...\n",
            "[Fooocus] Encoding negative #2 ...\n",
            "[Parameters] Denoising Strength = 1.0\n",
            "[Parameters] Initial Latent shape: Image Space (1152, 896)\n",
            "Preparation time: 3.48 seconds\n",
            "Using karras scheduler.\n",
            "[Fooocus] Preparing task 1/2 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "ERROR diffusion_model.input_blocks.8.0.in_layers.2.weight CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 26.12 MiB is free. Process 7002 has 14.71 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 471.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "ERROR diffusion_model.input_blocks.8.0.out_layers.3.weight CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 26.12 MiB is free. Process 7002 has 14.71 GiB memory in use. Of the allocated memory 14.15 GiB is allocated by PyTorch, and 440.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "ERROR diffusion_model.middle_block.0.in_layers.2.weight CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 26.12 MiB is free. Process 7002 has 14.71 GiB memory in use. Of the allocated memory 14.13 GiB is allocated by PyTorch, and 468.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "ERROR diffusion_model.middle_block.0.out_layers.3.weight CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 26.12 MiB is free. Process 7002 has 14.71 GiB memory in use. Of the allocated memory 14.16 GiB is allocated by PyTorch, and 437.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 1471, in worker\n",
            "    handler(task)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 1286, in handler\n",
            "    imgs, img_paths, current_progress = process_task(all_steps, async_task, callback, controlnet_canny_path,\n",
            "                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 295, in process_task\n",
            "    imgs = pipeline.process_diffusion(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/modules/default_pipeline.py\", line 379, in process_diffusion\n",
            "    sampled_latent = core.ksampler(\n",
            "                     ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/modules/core.py\", line 310, in ksampler\n",
            "    samples = ldm_patched.modules.sample.sample(model,\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/ldm_patched/modules/sample.py\", line 93, in sample\n",
            "    real_model, positive_copy, negative_copy, noise_mask, models = prepare_sampling(model, noise.shape, positive, negative, noise_mask)\n",
            "                                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/ldm_patched/modules/sample.py\", line 86, in prepare_sampling\n",
            "    ldm_patched.modules.model_management.load_models_gpu([model] + models, model.memory_required([noise_shape[0] * 2] + list(noise_shape[1:])) + inference_memory)\n",
            "  File \"/content/Fooocus/modules/patch.py\", line 447, in patched_load_models_gpu\n",
            "    y = ldm_patched.modules.model_management.load_models_gpu_origin(*args, **kwargs)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/ldm_patched/modules/model_management.py\", line 437, in load_models_gpu\n",
            "    cur_loaded_model = loaded_model.model_load(lowvram_model_memory)\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/ldm_patched/modules/model_management.py\", line 304, in model_load\n",
            "    raise e\n",
            "  File \"/content/Fooocus/ldm_patched/modules/model_management.py\", line 300, in model_load\n",
            "    self.real_model = self.model.patch_model(device_to=patch_model_to) #TODO: do something with loras and offloading to CPU\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/ldm_patched/modules/model_patcher.py\", line 202, in patch_model\n",
            "    out_weight = self.calculate_weight(self.patches[key], temp_weight, key).to(weight.dtype)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 26.12 MiB is free. Process 7002 has 14.71 GiB memory in use. Of the allocated memory 14.16 GiB is allocated by PyTorch, and 437.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Total time: 4.76 seconds\n",
            "Loaded preset: /content/Fooocus/presets/lcm.json\n",
            "[Parameters] Adaptive CFG = 7\n",
            "[Parameters] CLIP Skip = 2\n",
            "[Parameters] Sharpness = 2\n",
            "[Parameters] ControlNet Softness = 0.25\n",
            "[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n",
            "[Parameters] Seed = 5713092060727850105\n",
            "[Parameters] CFG = 4\n",
            "[Fooocus] Loading control models ...\n",
            "[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n",
            "[Parameters] Steps = 30 - 15\n",
            "[Fooocus] Initializing ...\n",
            "[Fooocus] Loading models ...\n",
            "Refiner unloaded.\n",
            "model_type EPS\n",
            "UNet ADM Dimension 2816\n",
            "Using pytorch attention in VAE\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "Using pytorch attention in VAE\n",
            "extra {'cond_stage_model.clip_l.logit_scale', 'cond_stage_model.clip_l.text_projection'}\n",
            "left over keys: dict_keys(['cond_stage_model.clip_l.transformer.text_model.embeddings.position_ids'])\n",
            "loaded straight to GPU\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "Base model loaded: /content/Fooocus/models/checkpoints/juggernautXL_v8Rundiffusion.safetensors\n",
            "VAE loaded: None\n",
            "Request to load LoRAs [('sd_xl_offset_example-lora_1.0.safetensors', 0.1)] for model [/content/Fooocus/models/checkpoints/juggernautXL_v8Rundiffusion.safetensors].\n",
            "Loaded LoRA [/content/Fooocus/models/loras/sd_xl_offset_example-lora_1.0.safetensors] for UNet [/content/Fooocus/models/checkpoints/juggernautXL_v8Rundiffusion.safetensors] with 788 keys at weight 0.1.\n",
            "Requested to load SDXLClipModel\n",
            "Requested to load GPT2LMHeadModel\n",
            "Loading 2 new models\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.95 seconds\n",
            "[Fooocus] Processing prompts ...\n",
            "[Fooocus] Preparing Fooocus text #1 ...\n",
            "[Prompt Expansion] Batmam, de copo inteiro, futuristic, color dynamic background, dramatic light, gorgeous, highly detailed, professional, strong, sharp focus, cool, shiny, symmetry, perfect, vivid, beautiful, advanced, romantic, enhanced, best, cute, creative, positive, unique, loving, attractive, pure, rational, artistic, new, exciting, thought\n",
            "[Fooocus] Preparing Fooocus text #2 ...\n",
            "[Prompt Expansion] Batmam, de copo inteiro, dramatic ambient light, futuristic, dynamic composition, atmosphere, detailed, intricate, full color, shiny, strong, sharp focus, professional fine detail, vivid, very inspirational, transparent background, inspired, beautiful, novel, perfect, expressive, artistic, new, romantic, surreal, creative, positive, unique, joyful, cute\n",
            "[Fooocus] Encoding positive #1 ...\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.12 seconds\n",
            "[Fooocus] Encoding positive #2 ...\n",
            "[Fooocus] Encoding negative #1 ...\n",
            "[Fooocus] Encoding negative #2 ...\n",
            "[Parameters] Denoising Strength = 1.0\n",
            "[Parameters] Initial Latent shape: Image Space (896, 1152)\n",
            "Preparation time: 45.24 seconds\n",
            "Using karras scheduler.\n",
            "[Fooocus] Preparing task 1/2 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 1.50 seconds\n",
            "100% 30/30 [00:23<00:00,  1.27it/s]\n",
            "Requested to load AutoencoderKL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.26 seconds\n",
            "[Fooocus] Saving image 1/2 to system ...\n",
            "Image generated with private log at: /content/Fooocus/outputs/2025-02-26/log.html\n",
            "Generating and saving time: 27.85 seconds\n",
            "[Fooocus] Preparing task 2/2 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.60 seconds\n",
            "100% 30/30 [00:24<00:00,  1.25it/s]\n",
            "Requested to load AutoencoderKL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.26 seconds\n",
            "[Fooocus] Saving image 2/2 to system ...\n",
            "Image generated with private log at: /content/Fooocus/outputs/2025-02-26/log.html\n",
            "Generating and saving time: 27.41 seconds\n",
            "[Enhance] Skipping, preconditions aren't met\n",
            "Processing time (total): 55.26 seconds\n",
            "Requested to load SDXLClipModel\n",
            "Requested to load GPT2LMHeadModel\n",
            "Loading 2 new models\n",
            "Total time: 100.55 seconds\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.79 seconds\n",
            "[Parameters] Adaptive CFG = 7\n",
            "[Parameters] CLIP Skip = 2\n",
            "[Parameters] Sharpness = 2\n",
            "[Parameters] ControlNet Softness = 0.25\n",
            "[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n",
            "[Parameters] Seed = 915197263937845461\n",
            "[Parameters] CFG = 4\n",
            "[Fooocus] Loading control models ...\n",
            "[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n",
            "[Parameters] Steps = 30 - 15\n",
            "[Fooocus] Initializing ...\n",
            "[Fooocus] Loading models ...\n",
            "Refiner unloaded.\n",
            "[Fooocus] Processing prompts ...\n",
            "[Fooocus] Preparing Fooocus text #1 ...\n",
            "[Prompt Expansion] 306 x 338, 0.91, 153:169, sweet wonderful delicate pretty striking gorgeous elegant lovely magical enchanted glossy polished translucent sacred light, stunning bright colorful intricate intense graceful glorious outstanding grand spectacular fine cinematic epic great magnificent excellent directed perfect focus shining fascinating beautiful charming magic marvelous pleasant highly detailed open incredible deep vibrant trendy full color neat fantastic sharp coherent symmetry extremely rich amazing awesome surreal pleasing vivid\n",
            "[Fooocus] Preparing Fooocus text #2 ...\n",
            "[Prompt Expansion] 306 x 338, 0.91, 153:169, sweet wonderful pure cute elegant lovely delicate pretty perfect detailed shining holy light great focus beautiful amazing fine detail very pleasing colors marvelous background vivid enormous glowing rich artistic excellent surreal incredible astonishing impressive outstanding fantastic supreme awesome spectacular exceptional astounding stunning exquisite phenomenal extraordinary brilliant splendid fabulous magnificent breathtaking magical fascinating epic singular delightful superb extremely enhanced gorgeous dazzling majestic pleasant highly\n",
            "[Fooocus] Encoding positive #1 ...\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.12 seconds\n",
            "[Fooocus] Encoding positive #2 ...\n",
            "[Fooocus] Encoding negative #1 ...\n",
            "[Fooocus] Encoding negative #2 ...\n",
            "[Parameters] Denoising Strength = 1.0\n",
            "[Parameters] Initial Latent shape: Image Space (896, 1152)\n",
            "Preparation time: 2.06 seconds\n",
            "Using karras scheduler.\n",
            "[Fooocus] Preparing task 1/2 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 1.24 seconds\n",
            "100% 30/30 [00:23<00:00,  1.25it/s]\n",
            "Requested to load AutoencoderKL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.27 seconds\n",
            "[Fooocus] Saving image 1/2 to system ...\n",
            "Image generated with private log at: /content/Fooocus/outputs/2025-02-26/log.html\n",
            "Generating and saving time: 27.95 seconds\n",
            "[Fooocus] Preparing task 2/2 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.58 seconds\n",
            "100% 30/30 [00:24<00:00,  1.21it/s]\n",
            "Requested to load AutoencoderKL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.26 seconds\n",
            "[Fooocus] Saving image 2/2 to system ...\n",
            "Image generated with private log at: /content/Fooocus/outputs/2025-02-26/log.html\n",
            "Generating and saving time: 28.00 seconds\n",
            "[Enhance] Skipping, preconditions aren't met\n",
            "Processing time (total): 55.95 seconds\n",
            "Requested to load SDXLClipModel\n",
            "Requested to load GPT2LMHeadModel\n",
            "Loading 2 new models\n",
            "Total time: 58.06 seconds\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.77 seconds\n",
            "[Parameters] Adaptive CFG = 7\n",
            "[Parameters] CLIP Skip = 2\n",
            "[Parameters] Sharpness = 2\n",
            "[Parameters] ControlNet Softness = 0.25\n",
            "[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n",
            "[Parameters] Seed = 6276533010663031438\n",
            "[Parameters] CFG = 4\n",
            "[Fooocus] Loading control models ...\n",
            "[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n",
            "[Parameters] Steps = 30 - 15\n",
            "[Fooocus] Initializing ...\n",
            "[Fooocus] Loading models ...\n",
            "Refiner unloaded.\n",
            "[Fooocus] Processing prompts ...\n",
            "[Fooocus] Preparing Fooocus text #1 ...\n",
            "[Prompt Expansion] Gato de potas, surreal, beautiful composition, glowing light atmosphere, full cinematic color, extremely detailed, intricate, elegant, dynamic, futuristic, sharp focus, vibrant colors, open intriguing, professional, winning, best, novel, romantic, stunning, inspired, attractive, cute, perfect, innocent, pretty, peaceful, thought, rich, epic, creative, positive\n",
            "[Fooocus] Preparing Fooocus text #2 ...\n",
            "[Prompt Expansion] Gato de potas, surreal, epic, cinematic, extremely detailed, very sharp focus, shining light deep, beautiful, stunning composition, cool color, intricate, elegant futuristic background, inspired, rich vivid colors, ambient, dynamic, cheerful, magical, thought complex, still, majestic, imposing, highly coherent, professional, protected, unique, great symmetry, perfect, innocent\n",
            "[Fooocus] Encoding positive #1 ...\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.13 seconds\n",
            "[Fooocus] Encoding positive #2 ...\n",
            "[Fooocus] Encoding negative #1 ...\n",
            "[Fooocus] Encoding negative #2 ...\n",
            "[Parameters] Denoising Strength = 1.0\n",
            "[Parameters] Initial Latent shape: Image Space (896, 1152)\n",
            "Preparation time: 2.08 seconds\n",
            "Using karras scheduler.\n",
            "[Fooocus] Preparing task 1/2 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 1.16 seconds\n",
            "100% 30/30 [00:24<00:00,  1.21it/s]\n",
            "Requested to load AutoencoderKL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.26 seconds\n",
            "[Fooocus] Saving image 1/2 to system ...\n",
            "Image generated with private log at: /content/Fooocus/outputs/2025-02-26/log.html\n",
            "Generating and saving time: 28.84 seconds\n",
            "[Fooocus] Preparing task 2/2 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.63 seconds\n",
            "100% 30/30 [00:24<00:00,  1.20it/s]\n",
            "Requested to load AutoencoderKL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.29 seconds\n",
            "[Fooocus] Saving image 2/2 to system ...\n",
            "Image generated with private log at: /content/Fooocus/outputs/2025-02-26/log.html\n",
            "Generating and saving time: 28.37 seconds\n",
            "[Enhance] Skipping, preconditions aren't met\n",
            "Processing time (total): 57.21 seconds\n",
            "Requested to load SDXLClipModel\n",
            "Requested to load GPT2LMHeadModel\n",
            "Loading 2 new models\n",
            "Total time: 59.34 seconds\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.76 seconds\n"
          ]
        }
      ],
      "source": [
        "!pip install pygit2==1.15.1\n",
        "%cd /content\n",
        "!git clone https://github.com/lllyasviel/Fooocus.git\n",
        "%cd /content/Fooocus\n",
        "!python entry_with_update.py --share --always-high-vram\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}